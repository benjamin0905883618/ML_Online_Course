{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML_OnlineCourse_HW5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7a78679489e3429daf6037df265ee216": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bf54b330a55e4bf5b36a8a1e41363329",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c02c45839ec8418bab1f1a335d0cf2a1",
              "IPY_MODEL_49cdba9b508b458384a66e81abade621"
            ]
          }
        },
        "bf54b330a55e4bf5b36a8a1e41363329": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c02c45839ec8418bab1f1a335d0cf2a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_86f73e0d31f848e3b9036f8ed6d4b415",
            "_dom_classes": [],
            "description": "train epoch 1:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 800,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_091c1376c46f42a5844efc8278500b78"
          }
        },
        "49cdba9b508b458384a66e81abade621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f0781d26a60541d882abc8b9692a9406",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/800 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_07cbd524bdf24d1a98db7ebdb00bb760"
          }
        },
        "86f73e0d31f848e3b9036f8ed6d4b415": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "091c1376c46f42a5844efc8278500b78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0781d26a60541d882abc8b9692a9406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "07cbd524bdf24d1a98db7ebdb00bb760": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tti52iZy2_HI"
      },
      "source": [
        "**Download and import library in need**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyp0sFoAUu0L",
        "outputId": "2e84493e-996f-470c-9da9-f185375b64e2"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Aug  4 07:47:31 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln5YBdmL2nUb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "228eed2f-58cb-4ae7-9994-0d73218b5b9b"
      },
      "source": [
        "!pip install 'torch>=1.6.0' editdistance matplotlib sacrebleu sacremoses sentencepiece tqdm wandb\n",
        "!pip install --upgrade jupyter ipywidgets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-1.5.1-py3-none-any.whl (54 kB)\n",
            "\u001b[?25l\r\u001b[K     |██████                          | 10 kB 35.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20 kB 23.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30 kB 17.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 40 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 51 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 54 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 15.5 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 80.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.41.1)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.11.2-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 76.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading portalocker-2.0.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.0.1)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting urllib3>=1.26.5\n",
            "  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 73.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 74.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 71.8 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting requests<3,>=2.0.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 967 kB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=ca59addf0b92ef673cc1d1a2242af8eb289d1a05772790c4edaee5f3bedadc3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=34ac4cd9c684578b6d015bb4414b425de7ca675ac85f05fe814c092dd93f231f\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, urllib3, gitdb, subprocess32, shortuuid, sentry-sdk, requests, portalocker, pathtools, GitPython, docker-pycreds, configparser, wandb, sentencepiece, sacremoses, sacrebleu\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed GitPython-3.1.18 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 portalocker-2.0.0 requests-2.26.0 sacrebleu-1.5.1 sacremoses-0.0.45 sentencepiece-0.1.96 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 urllib3-1.26.6 wandb-0.11.2\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.7/dist-packages (1.0.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (7.6.3)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter) (5.2.0)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter) (5.1.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter) (5.6.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter) (5.3.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter) (4.10.1)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.0.5)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (5.5.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets) (1.0.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter) (5.3.5)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (57.2.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipywidgets) (1.0.18)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets) (4.7.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets) (0.2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipywidgets) (1.15.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (1.7.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter) (2.11.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter) (2.8.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter) (22.1.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter) (2.0.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (3.3.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (0.3)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (1.4.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter) (0.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter) (21.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->bleach->nbconvert->jupyter) (2.4.7)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter) (1.9.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a79TU86E2737",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f6211b1-36a6-4f1c-f6a3-bfac287dcfb1"
      },
      "source": [
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "!cd fairseq && git checkout 9a1c497\n",
        "!pip install --upgrade ./fairseq/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 28924, done.\u001b[K\n",
            "remote: Total 28924 (delta 0), reused 0 (delta 0), pack-reused 28924\u001b[K\n",
            "Receiving objects: 100% (28924/28924), 12.12 MiB | 10.99 MiB/s, done.\n",
            "Resolving deltas: 100% (21725/21725), done.\n",
            "Note: checking out '9a1c497'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 9a1c4970 Make Hydra logging work with DDP (#1568)\n",
            "Processing ./fairseq\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (2019.12.20)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (1.14.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (1.19.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (1.9.0+cu102)\n",
            "Collecting hydra-core<1.1\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (0.29.23)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (1.5.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==1.0.0a0+9a1c497) (4.41.1)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1->fairseq==1.0.0a0+9a1c497) (5.2.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==1.0.0a0+9a1c497) (3.7.4.3)\n",
            "Collecting PyYAML>=5.1.*\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 13.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==1.0.0a0+9a1c497) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==1.0.0a0+9a1c497) (2.20)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1->fairseq==1.0.0a0+9a1c497) (3.5.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-1.0.0a0+9a1c497-cp37-cp37m-linux_x86_64.whl size=2851324 sha256=62e533302c5ccd9331fc9713127a0c9b8e1f6577040de26334de6e290035214b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cwrf1rmv/wheels/7c/35/80/edbd520a1a7e615df007002aeea9f6bf5f3c8f9243e072f6ce\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141229 sha256=6ff48bdf1a37a2161b2412d532c690bf5c5a6fbd3287e86ff76eacd525ffcfb8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: PyYAML, omegaconf, antlr4-python3-runtime, hydra-core, fairseq\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 antlr4-python3-runtime-4.8 fairseq-1.0.0a0+9a1c497 hydra-core-1.0.7 omegaconf-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufV9mXtw3FFO"
      },
      "source": [
        "import sys\n",
        "import pdb\n",
        "import pprint\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "import numpy as np\n",
        "import tqdm.auto as tqdm\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "from fairseq import utils\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwEmTsLL3hmp"
      },
      "source": [
        "**set seed**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naKpy21B3hJy"
      },
      "source": [
        "seed = 73\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nbD50Gy4Abm"
      },
      "source": [
        "**Download dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FJWoNiB4DHt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4a1b220-28f8-43d4-d0c6-fc6778bbe38f"
      },
      "source": [
        "!apt-get install megatools"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  megatools\n",
            "0 upgraded, 1 newly installed, 0 to remove and 40 not upgraded.\n",
            "Need to get 148 kB of archives.\n",
            "After this operation, 1,097 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 megatools amd64 1.9.98-1build2 [148 kB]\n",
            "Fetched 148 kB in 1s (194 kB/s)\n",
            "Selecting previously unselected package megatools.\n",
            "(Reading database ... 160837 files and directories currently installed.)\n",
            "Preparing to unpack .../megatools_1.9.98-1build2_amd64.deb ...\n",
            "Unpacking megatools (1.9.98-1build2) ...\n",
            "Setting up megatools (1.9.98-1build2) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTSTYzwZ4IJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b22aeb-386a-4663-c683-ed3c441b3dc7"
      },
      "source": [
        "data_dir = './DATA/rawdata'\n",
        "dataset_name = 'ted2020'\n",
        "urls = (\n",
        "#    '\"https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214989&authkey=AGgQ-DaR8eFSl1A\"', \n",
        "#    '\"https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214987&authkey=AA4qP_azsicwZZM\"',\n",
        "# # If the above links die, use the following instead. \n",
        "#     \"https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/ted2020.tgz\",\n",
        "#     \"https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/test.tgz\",\n",
        "# # If the above links die, use the following instead. \n",
        "     \"https://mega.nz/#!vEcTCISJ!3Rw0eHTZWPpdHBTbQEqBDikDEdFPr7fI8WxaXK9yZ9U\",\n",
        "     \"https://mega.nz/#!zNcnGIoJ!oPJX9AvVVs11jc0SaK6vxP_lFUNTkEcK2WbxJpvjU5Y\",\n",
        ")\n",
        "file_names = (\n",
        "  'ted2020.tgz', # train & dev\n",
        "  'test.tgz', # test\n",
        ")\n",
        "prefix = Path(data_dir).absolute() / dataset_name\n",
        "\n",
        "prefix.mkdir(parents=True, exist_ok=True)\n",
        "for u, f in zip(urls, file_names):\n",
        "  path = prefix/f\n",
        "  if not path.exists():\n",
        "    if 'mega' in u:\n",
        "      !megadl {u} --path {path}\n",
        "    else:\n",
        "      !wget {u} -O {path}\n",
        "  if path.suffix == \".tgz\":\n",
        "    !tar -xvf {path} -C {prefix}\n",
        "  elif path.suffix == \".zip\":\n",
        "    !unzip -o {path} -d {prefix}\n",
        "!mv {prefix/'raw.en'} {prefix/'train_dev.raw.en'}\n",
        "!mv {prefix/'raw.zh'} {prefix/'train_dev.raw.zh'}\n",
        "!mv {prefix/'test.en'} {prefix/'test.raw.en'}\n",
        "!mv {prefix/'test.zh'} {prefix/'test.raw.zh'}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0KDownloaded ted2020.tgz\n",
            "raw.en\n",
            "raw.zh\n",
            "\u001b[0KDownloaded test.tgz\n",
            "test.en\n",
            "test.zh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVi0Zy3F4L2W"
      },
      "source": [
        "**set language**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwYPgaoR4Nkz"
      },
      "source": [
        "src_lang = 'en'\n",
        "tgt_lang = 'zh'\n",
        "\n",
        "data_prefix = f'{prefix}/train_dev.raw'\n",
        "test_prefix = f'{prefix}/test.raw'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_FkxcRZ4Z0T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe8425a-4cf6-4ece-9eb8-2992726fa43f"
      },
      "source": [
        "!head {data_prefix + '.' + src_lang} -n 5\n",
        "!head {data_prefix + '.' + tgt_lang} -n 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thank you so much, Chris.\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\n",
            "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\n",
            "And I say that sincerely, partly because  I need that.\n",
            "Put yourselves in my position.\n",
            "非常謝謝你，克里斯。能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸。我非常感激。\n",
            "這個研討會給我留下了極為深刻的印象，我想感謝大家 對我之前演講的好評。\n",
            "我是由衷的想這麼說，有部份原因是因為 —— 我真的有需要!\n",
            "請你們設身處地為我想一想！\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZvMjfRA4lTX"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAprDicC4nDc"
      },
      "source": [
        "import re\n",
        "\n",
        "def strQ2B(ustring):\n",
        "  #把字串全形轉半形\n",
        "  ss = []\n",
        "  for s in ustring:\n",
        "    rstring = \"\"\n",
        "    for uchar in s:\n",
        "      inside_code = ord(uchar)\n",
        "      if inside_code == 12288: #全形空格直接轉換\n",
        "        inside_code = 32\n",
        "      elif (inside_code >= 65281) and inside_code <= 65374:\n",
        "        inside_code -= 65248\n",
        "      rstring += chr(inside_code)\n",
        "    ss.append(rstring)\n",
        "  return ''.join(ss)\n",
        "\n",
        "def clean_s(s, lang):\n",
        "  if lang == 'en':\n",
        "    s = re.sub(r\"\\([^()]*\\)\", \"\", s)\n",
        "    s = s.replace('-', '')\n",
        "    s = re.sub('([.,;~?()\\\"])', r' \\1 ', s) \n",
        "  elif lang == 'zh':\n",
        "    s = strQ2B(s)\n",
        "    s = re.sub(r\"\\([^()]*\\)\", \"\", s)\n",
        "    s = s.replace(' ', '')\n",
        "    s = s.replace('—', '')\n",
        "    s = s.replace('“', '\"')\n",
        "    s = s.replace('”', '\"')\n",
        "    s = s.replace('_', '')\n",
        "    s = re.sub('([。,;!?()\\\"~「」])', r' \\1 ', s) # keep punctuation\n",
        "  s = ' '.join(s.strip().split())\n",
        "  return s\n",
        "\n",
        "def len_s(s, lang):\n",
        "  if lang == 'zh':\n",
        "    return len(s)\n",
        "  return len(s.split())\n",
        "\n",
        "def clean_corpus(prefix, l1, l2, ratio = 9, max_len = 1000, min_len = 1):\n",
        "  if Path(f'{prefix}.clean.{l1}').exists() and Path(f'{prefix}.clean.{l2}').exists():\n",
        "    print(f'{prefix}.clean.{l1} & {l2} exists. skipping clean.')\n",
        "    return\n",
        "  with open(f'{prefix}.{l1}', 'r') as l1_in_f:\n",
        "    with open(f'{prefix}.{l2}', 'r') as l2_in_f:\n",
        "      with open(f'{prefix}.clean.{l1}', 'w') as l1_out_f:\n",
        "        with open(f'{prefix}.clean.{l2}', 'w') as l2_out_f:\n",
        "          for s1 in l1_in_f:\n",
        "            s1 = s1.strip()\n",
        "            s2 = l2_in_f.readline().strip()\n",
        "            s1 = clean_s(s1, l1)\n",
        "            s2 = clean_s(s2, l2)\n",
        "            s1_len = len_s(s1, l1)\n",
        "            s2_len = len_s(s2, l2)\n",
        "            if min_len > 0: # remove short sentence\n",
        "              if s1_len < min_len or s2_len < min_len:\n",
        "                continue\n",
        "            if max_len > 0: # remove long sentence\n",
        "              if s1_len > max_len or s2_len > max_len:\n",
        "                continue\n",
        "            if ratio > 0: # remove by ratio of length\n",
        "              if s1_len/s2_len > ratio or s2_len/s1_len > ratio:\n",
        "                continue\n",
        "            print(s1, file=l1_out_f)\n",
        "            print(s2, file=l2_out_f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtvEHfQJ62GZ"
      },
      "source": [
        "clean_corpus(data_prefix, src_lang, tgt_lang)\n",
        "clean_corpus(test_prefix, src_lang, tgt_lang, ratio=-1, min_len=-1, max_len=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS3hHf4R63uh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c52141-65d6-4d29-d2e5-6d615e72b19d"
      },
      "source": [
        "!head {data_prefix+'.clean.'+src_lang} -n 5\n",
        "!head {data_prefix+'.clean.'+tgt_lang} -n 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thank you so much , Chris .\n",
            "And it's truly a great honor to have the opportunity to come to this stage twice ; I'm extremely grateful .\n",
            "I have been blown away by this conference , and I want to thank all of you for the many nice comments about what I had to say the other night .\n",
            "And I say that sincerely , partly because I need that .\n",
            "Put yourselves in my position .\n",
            "非常謝謝你 , 克里斯 。 能有這個機會第二度踏上這個演講台\n",
            "真是一大榮幸 。 我非常感激 。\n",
            "這個研討會給我留下了極為深刻的印象 , 我想感謝大家對我之前演講的好評 。\n",
            "我是由衷的想這麼說 , 有部份原因是因為我真的有需要 !\n",
            "請你們設身處地為我想一想 !\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7NVby9e67DH"
      },
      "source": [
        "**Split data into trainset and validationset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTk4dxW37Cpf"
      },
      "source": [
        "valid_ratio = 0.01\n",
        "train_ratio = 1 - valid_ratio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bf2QgUfX7L1I"
      },
      "source": [
        "if (prefix/f'train.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'train.clean.{tgt_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{src_lang}').exists() \\\n",
        "and (prefix/f'valid.clean.{tgt_lang}').exists():\n",
        "  print(f'train/valid splits exists. skipping split.')\n",
        "else:\n",
        "  line_num = sum(1 for line in open(f'{data_prefix}.clean.{src_lang}'))\n",
        "  labels = list(range(line_num))\n",
        "  random.shuffle(labels)\n",
        "  for lang in [src_lang, tgt_lang]:\n",
        "    train_f = open(os.path.join(data_dir, dataset_name, f'train.clean.{lang}'), 'w')\n",
        "    valid_f = open(os.path.join(data_dir, dataset_name, f'valid.clean.{lang}'), 'w')\n",
        "    count = 0\n",
        "    for line in open(f'{data_prefix}.clean.{lang}', 'r'):\n",
        "      if labels[count]/line_num < train_ratio:\n",
        "        train_f.write(line)\n",
        "      else:\n",
        "        valid_f.write(line)\n",
        "        count += 1\n",
        "    train_f.close()\n",
        "    valid_f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2a3wXMXk7bJC"
      },
      "source": [
        "**Subword Units(斷詞單位,用於處理未登錄詞)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxXJZSgh7qmt"
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "vocab_size = 8000\n",
        "if (prefix/f'spm{vocab_size}.model').exists():\n",
        "  print(f'{prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
        "else:\n",
        "  spm.SentencePieceTrainer.train(\n",
        "    input = ','.join([f'{prefix}/train.clean.{src_lang}', f'{prefix}/valid.clean.{src_lang}', f'{prefix}/train.clean.{tgt_lang}', f'{prefix}/valid.clean.{tgt_lang}']),\n",
        "    model_prefix = prefix/f'spm{vocab_size}',\n",
        "    vocab_size = vocab_size,\n",
        "    character_coverage = 1,\n",
        "    model_type = 'unigram',\n",
        "    input_sentence_size = 1e6,\n",
        "    shuffle_input_sentence = True,\n",
        "    normalization_rule_name = 'nmt_nfkc_cf',\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofeWfCai8gzh"
      },
      "source": [
        "spm_model = spm.SentencePieceProcessor(model_file = str(prefix/f'spm{vocab_size}.model'))\n",
        "in_tag = {\n",
        "  'train': 'train.clean',\n",
        "  'valid': 'valid.clean',\n",
        "  'test': 'test.raw.clean',\n",
        "}\n",
        "for split in ['train', 'valid', 'test']:\n",
        "  for lang in [src_lang, tgt_lang]:\n",
        "    out_path = prefix/f'{split}.{lang}'\n",
        "    if out_path.exists():\n",
        "      print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "    else:\n",
        "      with open(prefix/f'{split}.{lang}', 'w') as out_f:\n",
        "        with open(prefix/f'{in_tag[split]}.{lang}', 'r') as in_f:\n",
        "          for line in in_f:\n",
        "            line = line.strip()\n",
        "            tok = spm_model.encode(line, out_type = str)\n",
        "            print(' '.join(tok), file = out_f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5fgKMApAYrY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5125b8f-2f56-4241-de57-726b1fbf6b94"
      },
      "source": [
        "!head {data_dir+'/'+dataset_name+'/train.'+src_lang} -n 5\n",
        "!head {data_dir+'/'+dataset_name+'/train.'+tgt_lang} -n 5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "▁thank ▁you ▁so ▁much ▁, ▁chris ▁.\n",
            "▁and ▁it ' s ▁tr u ly ▁a ▁great ▁ho n or ▁to ▁have ▁the ▁ op port un ity ▁to ▁come ▁to ▁this ▁st age ▁ t wi ce ▁; ▁i ' m ▁ex t re me ly ▁gr ate ful ▁.\n",
            "▁i ▁have ▁been ▁ bl own ▁away ▁by ▁this ▁con fer ence ▁, ▁and ▁i ▁want ▁to ▁thank ▁all ▁of ▁you ▁for ▁the ▁many ▁ ni ce ▁ com ment s ▁about ▁what ▁i ▁had ▁to ▁say ▁the ▁other ▁night ▁.\n",
            "▁and ▁i ▁say ▁that ▁since re ly ▁, ▁part ly ▁because ▁i ▁need ▁that ▁.\n",
            "▁put ▁your s el ve s ▁in ▁my ▁po s ition ▁.\n",
            "▁ 非常 謝 謝 你 ▁, ▁ 克 里 斯 ▁。 ▁ 能 有 這個 機會 第二 度 踏 上 這個 演講 台\n",
            "▁ 真 是 一 大 榮 幸 ▁。 ▁我 非常 感 激 ▁。\n",
            "▁這個 研 討 會 給我 留 下 了 極 為 深 刻 的 印 象 ▁, ▁我想 感 謝 大家 對我 之前 演講 的 好 評 ▁。\n",
            "▁我 是由 衷 的 想 這麼 說 ▁, ▁有 部份 原因 是因為 我 真的 有 需要 ▁!\n",
            "▁ 請 你們 設 身 處 地 為 我想 一 想 ▁!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NfQKjO0Awr0"
      },
      "source": [
        "**Transfer fairseq to Binary(bug紀錄)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNBLGyP4A1aP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "569be193-83e6-4dab-dd75-aa3ceacbe8a6"
      },
      "source": [
        "binpath = Path('./DATA/data-bin', dataset_name)\n",
        "if binpath.exists():\n",
        "  print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "  !python -m fairseq_cli.preprocess \\\n",
        "    --source-lang {src_lang}\\\n",
        "    --target-lang {tgt_lang}\\\n",
        "    --trainpref {prefix/'train'}\\\n",
        "    --validpref {prefix/'valid'}\\\n",
        "    --testpref {prefix/'test'}\\\n",
        "    --destdir {binpath}\\\n",
        "    --joined-dictionary\\\n",
        "    --workers 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-04 07:52:10 | INFO | fairseq_cli.preprocess | Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='DATA/data-bin/ted2020', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='en', srcdict=None, suppress_crashes=False, target_lang='zh', task='translation', tensorboard_logdir=None, testpref='/content/DATA/rawdata/ted2020/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/content/DATA/rawdata/ted2020/train', user_dir=None, validpref='/content/DATA/rawdata/ted2020/valid', wandb_project=None, workers=2)\n",
            "2021-08-04 07:52:45 | INFO | fairseq_cli.preprocess | [en] Dictionary: 8000 types\n",
            "2021-08-04 07:53:17 | INFO | fairseq_cli.preprocess | [en] /content/DATA/rawdata/ted2020/train.en: 393969 sents, 12330711 tokens, 0.0% replaced by <unk>\n",
            "2021-08-04 07:53:17 | INFO | fairseq_cli.preprocess | [en] Dictionary: 8000 types\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/preprocess.py\", line 398, in <module>\n",
            "    cli_main()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/preprocess.py\", line 394, in cli_main\n",
            "    main(args)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/preprocess.py\", line 284, in main\n",
            "    make_all(args.source_lang, src_dict)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/preprocess.py\", line 257, in make_all\n",
            "    vocab, validpref, outprefix, lang, num_workers=args.workers\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/preprocess.py\", line 248, in make_dataset\n",
            "    make_binary_dataset(vocab, input_prefix, output_prefix, lang, num_workers)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/fairseq_cli/preprocess.py\", line 181, in make_binary_dataset\n",
            "    100 * sum(replaced.values()) / n_seq_tok[1],\n",
            "ZeroDivisionError: division by zero\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4rI0HjiA8lL"
      },
      "source": [
        "**Set some hyperparameter**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Adf7K10A8Le"
      },
      "source": [
        "config = Namespace(\n",
        "  datadir = \"./DATA/data-bin/ted2020\",\n",
        "  savedir = \"./checkpoints/rnn\",\n",
        "  source_lang = \"en\",\n",
        "  target_lang = \"zh\",\n",
        "    \n",
        "  # cpu threads when fetching & processing data.\n",
        "  num_workers=2,  \n",
        "  # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
        "  max_tokens=8192,\n",
        "  accum_steps=2,\n",
        "    \n",
        "  # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
        "  lr_factor=2.,\n",
        "  lr_warmup=4000,\n",
        "    \n",
        "  # clipping gradient norm helps alleviate gradient exploding\n",
        "  clip_norm=1.0,\n",
        "    \n",
        "  # maximum epochs for training\n",
        "  max_epoch=25,\n",
        "  start_epoch=1,\n",
        "    \n",
        "  # beam size for beam search\n",
        "  beam=5, \n",
        "  # generate sequences of maximum length ax + b, where x is the source length\n",
        "  max_len_a=1.2, \n",
        "  max_len_b=10,\n",
        "  # when decoding, post process sentence by removing sentencepiece symbols.\n",
        "  post_process = \"sentencepiece\",\n",
        "    \n",
        "  # checkpoints\n",
        "  keep_last_epochs=5,\n",
        "  resume=None, # if resume from checkpoint name (under config.savedir)\n",
        "    \n",
        "  # logging\n",
        "  use_wandb=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnEqMs0WBNka"
      },
      "source": [
        "**Logging**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoTi2UFoBPPS"
      },
      "source": [
        "logging.basicConfig(\n",
        "  format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "  datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "  level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
        "  stream=sys.stdout,\n",
        ")\n",
        "proj = \"hw5.seq2seq\"\n",
        "logger = logging.getLogger(proj)\n",
        "if config.use_wandb:\n",
        "  import wandb\n",
        "  wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Aef9S6IBUFp"
      },
      "source": [
        "**Cuda environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f95lLJmeBiO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902dff5b-0454-4cd2-ab27-c8983b2cec6c"
      },
      "source": [
        "cuda_env = utils.CudaEnvironment()\n",
        "utils.CudaEnvironment.pretty_print_cuda_env_list([cuda_env])\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-04 09:09:17 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2021-08-04 09:09:17 | INFO | fairseq.utils | rank   0: capabilities =  6.0  ; total memory = 15.899 GB ; name = Tesla P100-PCIE-16GB                    \n",
            "2021-08-04 09:09:17 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enNRPz_wC357"
      },
      "source": [
        "**fairseq TranslationTask**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNOVrNkFC_RE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae77c1e-92ed-4795-e83c-82dab989db86"
      },
      "source": [
        "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
        "\n",
        "## setup task\n",
        "task_cfg = TranslationConfig(\n",
        "  data=config.datadir,\n",
        "  source_lang=config.source_lang,\n",
        "  target_lang=config.target_lang,\n",
        "  train_subset=\"train\",\n",
        "  required_seq_len_multiple=8,\n",
        "  dataset_impl=\"mmap\",\n",
        "  upsample_primary=1,\n",
        ")\n",
        "task = TranslationTask.setup_task(task_cfg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-04 09:09:18 | INFO | fairseq.tasks.translation | [en] dictionary: 8000 types\n",
            "2021-08-04 09:09:18 | INFO | fairseq.tasks.translation | [zh] dictionary: 8000 types\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFUPOpjJDLMo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f74604a-2168-4a20-a6e8-b21d28391d71"
      },
      "source": [
        "logger.info(\"loading data for epoch 1\")\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "task.load_dataset(split=\"valid\", epoch=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-04 09:09:20 | INFO | hw5.seq2seq | loading data for epoch 1\n",
            "2021-08-04 09:09:20 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020/train.en-zh.en\n",
            "2021-08-04 09:09:20 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020/train.en-zh.zh\n",
            "2021-08-04 09:09:20 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 train en-zh 390041 examples\n",
            "2021-08-04 09:09:20 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020/valid.en-zh.en\n",
            "2021-08-04 09:09:20 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020/valid.en-zh.zh\n",
            "2021-08-04 09:09:20 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 valid en-zh 3939 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3yDFVHUDgmr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "256d1dc3-d831-410f-f5b6-41c1957759fb"
      },
      "source": [
        "sample = task.dataset(\"valid\")[1]\n",
        "pprint.pprint(sample)\n",
        "pprint.pprint(\n",
        "  \"Source: \" + \\\n",
        "  task.source_dictionary.string(\n",
        "    sample['source'],\n",
        "    config.post_process,\n",
        "  )\n",
        ")\n",
        "pprint.pprint(\n",
        "  \"Target: \" + \\\n",
        "  task.target_dictionary.string(\n",
        "    sample['target'],\n",
        "    config.post_process,\n",
        "  )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'id': 1,\n",
            " 'source': tensor([  18,   14,    6, 2234,   60,   19,   80,    5,  256,   16,  405, 1407,\n",
            "        1706,    7,    2]),\n",
            " 'target': tensor([ 140,  690,   28,  270,   45,  151, 1142,  660,  606,  369, 3114, 2434,\n",
            "        1434,  192,    2])}\n",
            "\"Source: that's exactly what i do optical mind control .\"\n",
            "'Target: 這實在就是我所做的--光學操控思想'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zig2h_B5EF6w"
      },
      "source": [
        "**Dataset Iterator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJU_H1d6ENku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffff12f2-4911-4075-9de8-0d0d4245bffd"
      },
      "source": [
        "def load_data_iterator(task, split, epoch=1, max_tokens=4000, num_workers=1, cached=True):\n",
        "  batch_iterator = task.get_batch_iterator(\n",
        "    dataset=task.dataset(split),\n",
        "    max_tokens=max_tokens,\n",
        "    max_sentences=None,\n",
        "    max_positions=utils.resolve_max_positions(\n",
        "      task.max_positions(),\n",
        "      max_tokens,\n",
        "    ),\n",
        "    ignore_invalid_inputs=True,\n",
        "    seed=seed,\n",
        "    num_workers=num_workers,\n",
        "    epoch=epoch,\n",
        "    disable_iterator_cache=not cached,\n",
        "    # Set this to False to speed up. However, if set to False, changing max_tokens beyond \n",
        "    # first call of this method has no effect. \n",
        "  )\n",
        "  return batch_iterator\n",
        "\n",
        "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch=1, max_tokens=20, num_workers=1, cached=False)\n",
        "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle=True)\n",
        "sample = next(demo_iter)\n",
        "sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-04 09:09:23 | WARNING | fairseq.tasks.fairseq_task | 2,532 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[29, 135, 2444, 3058, 682, 731, 235, 1558, 3383, 559]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': tensor([723]),\n",
              " 'net_input': {'prev_output_tokens': tensor([[   2,  140,  296,  318, 1560,   51,  568,  316,  225, 1952,  254,   78,\n",
              "            151, 2691,    9,  215, 1680,   10,    1,    1,    1,    1,    1,    1]]),\n",
              "  'src_lengths': tensor([19]),\n",
              "  'src_tokens': tensor([[   1,    1,    1,    1,    1,   18,   26,   82,    8,  480,   15,  651,\n",
              "           1361,   38,    6,  176, 2696,   39,    5,  822,   92,  260,    7,    2]])},\n",
              " 'nsentences': 1,\n",
              " 'ntokens': 18,\n",
              " 'target': tensor([[ 140,  296,  318, 1560,   51,  568,  316,  225, 1952,  254,   78,  151,\n",
              "          2691,    9,  215, 1680,   10,    2,    1,    1,    1,    1,    1,    1]])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aszdhL9dEg-L"
      },
      "source": [
        "**define model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RJPH94QEoda"
      },
      "source": [
        "from fairseq.models import(\n",
        "  FairseqEncoder,\n",
        "  FairseqIncrementalDecoder,\n",
        "  FairseqEncoderDecoderModel\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhNmkeMuFAHj"
      },
      "source": [
        "**Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt2c7cEEFC-n"
      },
      "source": [
        "class RNNEncoder(FairseqEncoder):\n",
        "  def __init__(self, args, dictionary, embed_tokens):\n",
        "    super().__init__(dictionary)\n",
        "    self.embed_tokens = embed_tokens\n",
        "        \n",
        "    self.embed_dim = args.encoder_embed_dim\n",
        "    self.hidden_dim = args.encoder_ffn_embed_dim\n",
        "    self.num_layers = args.encoder_layers\n",
        "        \n",
        "    self.dropout_in_module = nn.Dropout(args.dropout)\n",
        "    self.rnn = nn.GRU(\n",
        "      self.embed_dim, \n",
        "      self.hidden_dim, \n",
        "      self.num_layers, \n",
        "      dropout=args.dropout, \n",
        "      batch_first=False, \n",
        "      bidirectional=True\n",
        "    )\n",
        "    self.dropout_out_module = nn.Dropout(args.dropout)\n",
        "        \n",
        "    self.padding_idx = dictionary.pad()\n",
        "        \n",
        "  def combine_bidir(self, outs, bsz: int):\n",
        "    out = outs.view(self.num_layers, 2, bsz, -1).transpose(1, 2).contiguous()\n",
        "    return out.view(self.num_layers, bsz, -1)\n",
        "\n",
        "  def forward(self, src_tokens, **unused):\n",
        "    bsz, seqlen = src_tokens.size()\n",
        "        \n",
        "    # get embeddings\n",
        "    x = self.embed_tokens(src_tokens)\n",
        "    x = self.dropout_in_module(x)\n",
        "\n",
        "    # B x T x C -> T x B x C\n",
        "    x = x.transpose(0, 1)\n",
        "        \n",
        "    # 過雙向RNN\n",
        "    h0 = x.new_zeros(2 * self.num_layers, bsz, self.hidden_dim)\n",
        "    x, final_hiddens = self.rnn(x, h0)\n",
        "    outputs = self.dropout_out_module(x)\n",
        "    # outputs = [sequence len, batch size, hid dim * directions] 是最上層RNN的輸出\n",
        "    # hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "        \n",
        "    # 因為 Encoder 是雙向的RNN，所以需要將同一層兩個方向的 hidden state 接在一起\n",
        "    final_hiddens = self.combine_bidir(final_hiddens, bsz)\n",
        "    # hidden =  [num_layers x batch x num_directions*hidden]\n",
        "        \n",
        "    encoder_padding_mask = src_tokens.eq(self.padding_idx).t()\n",
        "    return tuple(\n",
        "        (\n",
        "          outputs,  # seq_len x batch x hidden\n",
        "          final_hiddens,  # num_layers x batch x num_directions*hidden\n",
        "          encoder_padding_mask,  # seq_len x batch\n",
        "        )\n",
        "    )\n",
        "    \n",
        "  def reorder_encoder_out(self, encoder_out, new_order):\n",
        "    # 這個beam search時會用到，意義並不是很重要\n",
        "    return tuple(\n",
        "        (\n",
        "          encoder_out[0].index_select(1, new_order),\n",
        "          encoder_out[1].index_select(1, new_order),\n",
        "          encoder_out[2].index_select(1, new_order),\n",
        "        )\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5oKxu8oF6re"
      },
      "source": [
        "**Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdilCrdxF8dj"
      },
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "  def __init__(self, input_embed_dim, source_embed_dim, output_embed_dim, bias=False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_proj = nn.Linear(input_embed_dim, source_embed_dim, bias=bias)\n",
        "    self.output_proj = nn.Linear(\n",
        "      input_embed_dim + source_embed_dim, output_embed_dim, bias=bias\n",
        "    )\n",
        "\n",
        "  def forward(self, inputs, encoder_outputs, encoder_padding_mask):\n",
        "    # inputs: T, B, dim\n",
        "    # encoder_outputs: S x B x dim\n",
        "    # padding mask:  S x B\n",
        "        \n",
        "    # convert all to batch first\n",
        "    inputs = inputs.transpose(1,0) # B, T, dim\n",
        "    encoder_outputs = encoder_outputs.transpose(1,0) # B, S, dim\n",
        "    encoder_padding_mask = encoder_padding_mask.transpose(1,0) # B, S\n",
        "        \n",
        "    # 投影到encoder_outputs的維度\n",
        "    x = self.input_proj(inputs)\n",
        "\n",
        "    # 計算attention\n",
        "    # (B, T, dim) x (B, dim, S) = (B, T, S)\n",
        "    attn_scores = torch.bmm(x, encoder_outputs.transpose(1,2))\n",
        "\n",
        "    # 擋住padding位置的attention\n",
        "    if encoder_padding_mask is not None:\n",
        "      # 利用broadcast  B, S -> (B, 1, S)\n",
        "      encoder_padding_mask = encoder_padding_mask.unsqueeze(1)\n",
        "      attn_scores = (\n",
        "          attn_scores.float()\n",
        "          .masked_fill_(encoder_padding_mask, float(\"-inf\"))\n",
        "          .type_as(attn_scores)\n",
        "      )  # FP16 support: cast to float and back\n",
        "\n",
        "    # 在source對應維度softmax\n",
        "    attn_scores = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "    # 形狀 (B, T, S) x (B, S, dim) = (B, T, dim) 加權平均\n",
        "    x = torch.bmm(attn_scores, encoder_outputs)\n",
        "\n",
        "    # (B, T, dim)\n",
        "    x = torch.cat((x, inputs), dim=-1)\n",
        "    x = torch.tanh(self.output_proj(x)) # concat + linear + tanh\n",
        "        \n",
        "    # 回復形狀 (B, T, dim) -> (T, B, dim)\n",
        "    return x.transpose(1,0), attn_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smktza0VGZb0"
      },
      "source": [
        "**Decoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2V81sVwFGbIE"
      },
      "source": [
        "class RNNDecoder(FairseqIncrementalDecoder):\n",
        "  def __init__(self, args, dictionary, embed_tokens):\n",
        "    super().__init__(dictionary)\n",
        "    self.embed_tokens = embed_tokens\n",
        "        \n",
        "    assert args.decoder_layers == args.encoder_layers, f\"\"\"seq2seq rnn requires that encoder \n",
        "    and decoder have same layers of rnn. got: {args.encoder_layers, args.decoder_layers}\"\"\"\n",
        "    assert args.decoder_ffn_embed_dim == args.encoder_ffn_embed_dim*2, f\"\"\"seq2seq-rnn requires \n",
        "    that decoder hidden to be 2*encoder hidden dim. got: {args.decoder_ffn_embed_dim, args.encoder_ffn_embed_dim*2}\"\"\"\n",
        "        \n",
        "    self.embed_dim = args.decoder_embed_dim\n",
        "    self.hidden_dim = args.decoder_ffn_embed_dim\n",
        "    self.num_layers = args.decoder_layers\n",
        "        \n",
        "        \n",
        "    self.dropout_in_module = nn.Dropout(args.dropout)\n",
        "    self.rnn = nn.GRU(\n",
        "      self.embed_dim, \n",
        "      self.hidden_dim, \n",
        "      self.num_layers, \n",
        "      dropout=args.dropout, \n",
        "      batch_first=False, \n",
        "      bidirectional=False\n",
        "    )\n",
        "    self.attention = AttentionLayer(\n",
        "      self.embed_dim, self.hidden_dim, self.embed_dim, bias=False\n",
        "    ) \n",
        "    # self.attention = None\n",
        "    self.dropout_out_module = nn.Dropout(args.dropout)\n",
        "        \n",
        "    if self.hidden_dim != self.embed_dim:\n",
        "      self.project_out_dim = nn.Linear(self.hidden_dim, self.embed_dim)\n",
        "    else:\n",
        "      self.project_out_dim = None\n",
        "        \n",
        "    if args.share_decoder_input_output_embed:\n",
        "      self.output_projection = nn.Linear(\n",
        "        self.embed_tokens.weight.shape[1],\n",
        "        self.embed_tokens.weight.shape[0],\n",
        "        bias=False,\n",
        "      )\n",
        "      self.output_projection.weight = self.embed_tokens.weight\n",
        "    else:\n",
        "      self.output_projection = nn.Linear(\n",
        "        self.output_embed_dim, len(dictionary), bias=False\n",
        "      )\n",
        "      nn.init.normal_(\n",
        "        self.output_projection.weight, mean=0, std=self.output_embed_dim ** -0.5\n",
        "      )\n",
        "        \n",
        "  def forward(self, prev_output_tokens, encoder_out, incremental_state=None, **unused):\n",
        "    # 取出encoder的輸出\n",
        "    encoder_outputs, encoder_hiddens, encoder_padding_mask = encoder_out\n",
        "    # outputs:          seq_len x batch x num_directions*hidden\n",
        "    # encoder_hiddens:  num_layers x batch x num_directions*encoder_hidden\n",
        "    # padding_mask:     seq_len x batch\n",
        "        \n",
        "    if incremental_state is not None and len(incremental_state) > 0:\n",
        "      # 有上個timestep留下的資訊，讀進來就可以繼續decode，不用從bos重來\n",
        "      prev_output_tokens = prev_output_tokens[:, -1:]\n",
        "      cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
        "      prev_hiddens = cache_state[\"prev_hiddens\"]\n",
        "    else:\n",
        "      # 沒有incremental state代表這是training或者是test time時的第一步\n",
        "      # 準備seq2seq: 把encoder_hiddens pass進去decoder的hidden states\n",
        "      prev_hiddens = encoder_hiddens\n",
        "        \n",
        "    bsz, seqlen = prev_output_tokens.size()\n",
        "        \n",
        "    # embed tokens\n",
        "    x = self.embed_tokens(prev_output_tokens)\n",
        "    x = self.dropout_in_module(x)\n",
        "\n",
        "    # B x T x C -> T x B x C\n",
        "    x = x.transpose(0, 1)\n",
        "                \n",
        "    # 做decoder-to-encoder attention\n",
        "    if self.attention is not None:\n",
        "      x, attn = self.attention(x, encoder_outputs, encoder_padding_mask)\n",
        "                        \n",
        "    # 過單向RNN\n",
        "    x, final_hiddens = self.rnn(x, prev_hiddens)\n",
        "    # outputs = [sequence len, batch size, hid dim]\n",
        "    # hidden =  [num_layers * directions, batch size  , hid dim]\n",
        "    x = self.dropout_out_module(x)\n",
        "                \n",
        "    # 投影到embedding size (如果hidden 和embed size不一樣，然後share_embedding又設成True,需要額外project一次)\n",
        "    if self.project_out_dim != None:\n",
        "      x = self.project_out_dim(x)\n",
        "        \n",
        "    # 投影到vocab size 的分佈\n",
        "    x = self.output_projection(x)\n",
        "        \n",
        "    # T x B x C -> B x T x C\n",
        "    x = x.transpose(1, 0)\n",
        "        \n",
        "    # 如果是Incremental, 記錄這個timestep的hidden states, 下個timestep讀回來\n",
        "    cache_state = {\n",
        "      \"prev_hiddens\": final_hiddens,\n",
        "    }\n",
        "    self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
        "        \n",
        "    return x, None\n",
        "    \n",
        "  def reorder_incremental_state(self, incremental_state, new_order):\n",
        "    # 這個beam search時會用到，意義並不是很重要\n",
        "    cache_state = self.get_incremental_state(incremental_state, \"cached_state\")\n",
        "    prev_hiddens = cache_state[\"prev_hiddens\"]\n",
        "    prev_hiddens = [p.index_select(0, new_order) for p in prev_hiddens]\n",
        "    cache_state = {\n",
        "      \"prev_hiddens\": torch.stack(prev_hiddens),\n",
        "    }\n",
        "    self.set_incremental_state(incremental_state, \"cached_state\", cache_state)\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVUaNK3ZHY03"
      },
      "source": [
        "**Seq2seq**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KySGheJcHahl"
      },
      "source": [
        "class Seq2Seq(FairseqEncoderDecoderModel):\n",
        "  def __init__(self, args, encoder, decoder):\n",
        "    super().__init__(encoder, decoder)\n",
        "    self.args = args\n",
        "  def forward(self, src_tokens, src_lengths, prev_output_tokens, return_all_hiddens : bool = True):\n",
        "    #Run the forward pass for an encoder-decoder model\n",
        "    encoder_out = self.encoder(\n",
        "      src_tokens, src_lengths = src_lengths, return_all_hiddens = return_all_hiddens\n",
        "    )\n",
        "    logits, extra = self.decoder(\n",
        "      prev_output_tokens, encoder_out = encoder_out, src_lengths = src_lengths, return_all_hiddens = return_all_hiddens\n",
        "    )\n",
        "    return logits, extra"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEuMEx2GJILY"
      },
      "source": [
        "**Initialize Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brApEslCJMN5"
      },
      "source": [
        "from fairseq.models.transformer import TransformerEncoder, TransformerDecoder\n",
        "\n",
        "def build_model(args, task):\n",
        "  #按照參數設定建置模型\n",
        "  src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "  #Embed tokens\n",
        "  encoder_embed_tokens = nn.Embedding(len(src_dict), args.encoder_embed_dim, src_dict.pad())\n",
        "  decoder_embed_tokens = nn.Embedding(len(tgt_dict), args.decoder_embed_dim, tgt_dict.pad())\n",
        "\n",
        "  encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
        "  decoder = TransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "\n",
        "  model = Seq2Seq(args, encoder, decoder)\n",
        "\n",
        "  def init_params(module):\n",
        "    from fairseq.modules import MultiheadAttention\n",
        "    if isinstance(module, nn.Linear):\n",
        "      module.weight.data.normal_(mean = 0.0, std = 0.02)\n",
        "      if module.bias is not None:\n",
        "        module.bias.data.zero_()\n",
        "    if isinstance(module, nn.Embedding):\n",
        "      module.weight.data.normal_(mean = 0.0, std = 0.02)\n",
        "      if module.padding_idx is not None:\n",
        "        module.weight.data[module.padding_idx].zero_()\n",
        "    if isinstance(module, MultiheadAttention):\n",
        "      module.q_proj.weight.data.normal_(mean = 0.0, std = 0.02)\n",
        "      module.k_proj.weight.data.normal_(mean = 0.0, std = 0.02)\n",
        "      module.v_proj.weight.data.normal_(mean = 0.0, std = 0.02)\n",
        "    if isinstance(module, nn.RNNBase):\n",
        "      for name, param in module.named_parameters():\n",
        "        if \"weight\" in  name or \"bias\" in name:\n",
        "          param.data.uniform_(-0.1, 0.1)\n",
        "  model.apply(init_params)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoNEiomlST-t"
      },
      "source": [
        "![attention_is_all_you_need_table3.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co_wQNp6Kj1R"
      },
      "source": [
        "arch_args = Namespace(\n",
        "  encoder_embed_dim = 1024,\n",
        "  encoder_ffn_embed_dim = 4096,\n",
        "  encoder_layers = 6,\n",
        "  decoder_embed_dim = 1024,\n",
        "  decoder_ffn_embed_dim = 4096,\n",
        "  decoder_layers = 6,\n",
        "  share_decoder_input_output_embed = True,\n",
        "  dropout = 0.3,\n",
        ")\n",
        "\n",
        "def add_transformer_args(args):\n",
        "  args.encoder_attention_heads = 16\n",
        "  args.encoder_normalize_before = True\n",
        "\n",
        "  args.decoder_attention_heads = 16\n",
        "  args.decoder_normalize_before = True\n",
        "\n",
        "  args.activation_fn = \"relu\"\n",
        "  args.max_source_positions = 1024\n",
        "  args.max_target_positions = 1024\n",
        "\n",
        "  from fairseq.models.transformer import base_architecture\n",
        "  base_architecture(arch_args)\n",
        "  \n",
        "add_transformer_args(arch_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UztNI3tLTcuj"
      },
      "source": [
        "if config.use_wandb:\n",
        "  wandb.config.update(vars(arch_args))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwTQPbURXLiM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc5057c3-8711-4e72-cb24-7dd1a352437e"
      },
      "source": [
        "model = build_model(arch_args, task)\n",
        "logger.info(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-04 09:09:45 | INFO | hw5.seq2seq | Seq2Seq(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(8000, 1024, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(8000, 1024, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=1024, out_features=8000, bias=False)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnTfq-rpXUWS"
      },
      "source": [
        "**Label Smooth CrossEntropy Criterion**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkDAZZr2XeCd"
      },
      "source": [
        "class LabelSmoothedCrossEntropyCriterion(nn.Module):\n",
        "  def __init__(self, smoothing, ignore_index = None, reduce = True):\n",
        "    super().__init__()\n",
        "    self.smoothing = smoothing\n",
        "    self.ignore_index = ignore_index\n",
        "    self.reduce = reduce\n",
        "  def forward(self, lprobs, target):\n",
        "    if target.dim() == lprobs.dim() - 1:\n",
        "      target = target.unsqueeze(-1)\n",
        "    #Negative log likelihood, 當目標是one-hot時的crossentropy loss\n",
        "    nll_loss = -lprobs.gather(dim = -1, index = target)\n",
        "    #將一部分正確答案的機率分配給其他label 所以當計算crossentropy時等於把所有label的log prob加起來\n",
        "    smooth_loss = -lprobs.sum(dim = -1, keepdim = True)\n",
        "    if self.ignore_index is not None:\n",
        "      pad_mask = target.eq(self.ignore_index)\n",
        "      nll_loss.masked_fill_(pad_mask, 0.0)\n",
        "      smooth_loss.masked_fill_(pad_mask, 0.0)\n",
        "    else:\n",
        "      nll_loss = nll_loss.squeeze(-1)\n",
        "      smooth_loss = smooth_loss.squeeze(-1)\n",
        "    if self.reduce:\n",
        "      nll_loss = nll_loss.sum()\n",
        "      smooth_loss = smooth_loss.sum()\n",
        "    eps_i = self.smoothing / lprobs.size(-1)\n",
        "    loss = (1.0 - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
        "    return loss\n",
        "\n",
        "criterion = LabelSmoothedCrossEntropyCriterion(smoothing = 0.1, ignore_index = task.target_dictionary.pad())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmY3j1P4bGhj"
      },
      "source": [
        "**Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzhCRTXWXqdx"
      },
      "source": [
        "class NoamOpt:\n",
        "  \"Optim wrapper that implements rate.\"\n",
        "  def __init__(self, model_size, factor, warmup, optimizer):\n",
        "    self.optimizer = optimizer\n",
        "    self._step = 0\n",
        "    self.warmup = warmup\n",
        "    self.factor = factor\n",
        "    self.model_size = model_size\n",
        "    self._rate = 0\n",
        "    \n",
        "  @property\n",
        "  def param_groups(self):\n",
        "    return self.optimizer.param_groups\n",
        "        \n",
        "  def multiply_grads(self, c):\n",
        "    \"\"\"Multiplies grads by a constant *c*.\"\"\"                \n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        if p.grad is not None:\n",
        "          p.grad.data.mul_(c)\n",
        "        \n",
        "  def step(self):\n",
        "    \"Update parameters and rate\"\n",
        "    self._step += 1\n",
        "    rate = self.rate()\n",
        "    for p in self.param_groups:\n",
        "      p['lr'] = rate\n",
        "    self._rate = rate\n",
        "    self.optimizer.step()\n",
        "        \n",
        "  def rate(self, step = None):\n",
        "    \"Implement `lrate` above\"\n",
        "    if step is None:\n",
        "      step = self._step\n",
        "    return 0 if not step else self.factor * \\\n",
        "        (self.model_size ** (-0.5) *\n",
        "        min(step ** (-0.5), step * self.warmup ** (-1.5)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7jZcjRJbEyT"
      },
      "source": [
        "**排程視覺化**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6aJSmkpbN1s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "b61e4683-6b61-4222-acc9-353baa15b83a"
      },
      "source": [
        "optimizer = NoamOpt(\n",
        "  model_size=arch_args.encoder_embed_dim, \n",
        "  factor=config.lr_factor, \n",
        "  warmup=config.lr_warmup, \n",
        "  optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
        "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
        "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])\n",
        "None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ0mvUyzGtqa"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlxJBQreDW-q"
      },
      "source": [
        "from fairseq.data import iterators\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "def train_one_epoch(epoch_itr, model, task, criterion, optimizer, accum_steps = 1):\n",
        "  itr = epoch_itr.next_epoch_itr(shuffle = True)\n",
        "  itr = iterators.GroupedIterator(itr, accum_steps) #梯度累積\n",
        "\n",
        "  stats = {\"loss\": []}\n",
        "  scaler = GradScaler() #混和精度訓練\n",
        "\n",
        "  model.train()\n",
        "  progress = tqdm.tqdm(itr, desc = f\"train epoch {epoch_itr.epoch}\", leave = False)\n",
        "  for samples in progress:\n",
        "    model.zero_grad()\n",
        "    accum_loss = 0\n",
        "    sample_size = 0\n",
        "    \n",
        "    #梯度累積\n",
        "    for i, sample in enumerate(samples):\n",
        "      if i == 1:\n",
        "        torch.cuda.empty_cache()\n",
        "      sample = utils.move_to_cuda(sample, device = device)\n",
        "      target = sample[\"target\"]\n",
        "      sample_size_i = sample[\"ntokens\"]\n",
        "      sample_size += sample_size_i\n",
        "\n",
        "      #混和精度訓練\n",
        "      with autocast():\n",
        "        net_output = model.forward(**sample[\"net_input\"])\n",
        "        lprobs = F.log_softmax(net_output[0], -1)\n",
        "        loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1))\n",
        "\n",
        "        #logging\n",
        "        accum_loss += loss.item()\n",
        "        #back-prop\n",
        "        scaler.scale(loss).backward()\n",
        "    scaler.unscale_(optimizer)\n",
        "    optimizer.multiply_grads(1 / (sample_size or 1.0))\n",
        "    gnorm = nn.utils.clip_grad_norm_(model.parameters(), config.clip_norm) #梯度裁減, 避免梯度爆炸\n",
        "\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    loss_print = accum_loss / sample_size\n",
        "    stats[\"loss\"].append(loss_print)\n",
        "    progress.set_postfix(loss = loss_print)\n",
        "    if config.use_wandb:\n",
        "      wandb.log({\n",
        "          \"train/loss\": loss_print,\n",
        "          \"train/grad_norm\": gnorm.item(),\n",
        "          \"train/lr\": optimizer.rate(),\n",
        "          \"train/sample_size\": sample_size,\n",
        "      })\n",
        "  loss_print = np.mean(stats[\"loss\"])\n",
        "  logger.info(f\"training loss: {loss_print:.4f}\")\n",
        "  return stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSa0d0tbGwAC"
      },
      "source": [
        "**Validation and Inference**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdFewE2AGrwt"
      },
      "source": [
        "sequence_generator = task.build_generator([model], config)\n",
        "def decode(toks, dictionary):\n",
        "  s = dictionary.string(\n",
        "    toks.int().cpu(),\n",
        "    config.post_process,\n",
        "  )\n",
        "def inference_step(sample, model):\n",
        "  gen_out = sequence_generator.generate([model], sample)\n",
        "  srcs = []\n",
        "  hyps = []\n",
        "  refs = []\n",
        "  for i in range(len(gen_out)):\n",
        "    srcs.append(decode(\n",
        "      utils.strip_pad(sample[\"net_input\"][\"src_tokens\"][i], task.source_dictionary.pad()),\n",
        "      task.source_dictionary,\n",
        "    ))\n",
        "    hyps.append(decode(\n",
        "      gen_out[i][0][\"token\"],\n",
        "      task.target_dictionary,\n",
        "    ))\n",
        "    refs.append(decode(\n",
        "      utils.strip_pad(sample[\"target\"][i], task.target_dictionary.pad()),\n",
        "      task.target_dictionary,\n",
        "    ))\n",
        "  return srcs, hyps, refs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFNFPdoSIN4y"
      },
      "source": [
        "import shutil\n",
        "import sacrebleu\n",
        "\n",
        "def validate(model, task, criterion, log_to_wandb = True):\n",
        "  logger.info('begin validation')\n",
        "  itr = load_data_iterator(task, \"valid\", 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle = False)\n",
        "\n",
        "  stats = {\"loss\": [], \"bleu\": 0, \"srcs\": [], \"hyps\": [], \"refs\": []}\n",
        "  srcs = []\n",
        "  hyps = []\n",
        "  refs = []\n",
        "\n",
        "  model.eval()\n",
        "  progress = tqdm.tqdm(itr, desc = f\"validation\", leave = False)\n",
        "  with torch.no_grad():\n",
        "    for i, sample in enumerate(progress):\n",
        "      #validation loss\n",
        "      sample = utils.move_to_cuda(sample, device = device)\n",
        "      net_output = model.forward(**sample[\"net_input\"])\n",
        "\n",
        "      lprobs = F.log_softmax(net_output[0], 1)\n",
        "      target = sample[\"target\"]\n",
        "      sample_size = sample[\"ntokens\"]\n",
        "      loss = criterion(lprobs.view(-1, lprobs.size(-1)), target.view(-1)) / sample_size\n",
        "      progress.set_postfix(valid_loss = loss.item())\n",
        "      stats[\"loss\"].append(loss)\n",
        "\n",
        "      s, h, r = inference_step(sample, model)\n",
        "      srcs.extend(s)\n",
        "      hyps.extend(h)\n",
        "      refs.extend(r)\n",
        "    tok = 'zh' if task.cfg.target_lang == 'zh' else '13a'\n",
        "    stats[\"loss\"] = torch.stack(stats[\"loss\"]).mean().item()\n",
        "    stats[\"bleu\"] = sacreble.corpus_bleu(hyps, [refs], tokenize = tok)\n",
        "    stats[\"srcs\"] = srcs\n",
        "    stats[\"hyps\"] = hyps\n",
        "    stats[\"refs\"] = refs\n",
        "\n",
        "    if config.use_wandb and log_to_wandb:\n",
        "      wandb.log({\n",
        "        \"valid/loss\": stats[\"loss\"],\n",
        "        \"valid/bleu\": stats[\"bleu\"].score,\n",
        "      }, commit = False)\n",
        "    showid = np.random.randint(len(hyps))\n",
        "    logger.info(\"example source:\" + srcs[showid])\n",
        "    logger.info(\"example hypothesis:\" + hyps[showid])\n",
        "    logger.info(\"example reference:\" + refs[showid])\n",
        "\n",
        "    #show bleu results\n",
        "    logger.info(f\"validation loss:\\t{stats['loss']:.4f}\")\n",
        "    logger.info(stats[\"bleu\"].format())\n",
        "    return stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fb_dopmyLQlk"
      },
      "source": [
        "**save and load parameter of model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_QvbhnvLQC1"
      },
      "source": [
        "def validation_and_save(model, task, criterion, optimizer, epoch, save = True):\n",
        "  stats = validate(model, task, criterion)\n",
        "  bleu = stats['bleu']\n",
        "  loss = stats['loss']\n",
        "  if save:\n",
        "    savedir = Path(config.savedir).absolute()\n",
        "    savedir.mkdir(parents = True, exist_ok = True)\n",
        "\n",
        "    check = {\n",
        "      \"model\": model.state_dict(),\n",
        "      \"stats\": {\"bleu\": bleu.score, \"loss\": loss},\n",
        "      \"optim\": {\"step\": optimizer._step}\n",
        "    }\n",
        "    torch.save(check, savedir/f\"checkpoint{epoch}.pt\")\n",
        "    shutil.copy(savedir/f\"checkpoint{epoch}.pt\", savedir/f\"checkpoint_last.pt\")\n",
        "    logger.info(f\"saved epoch checkpoint: {savedir}/checkpoint{epoch}.pt\")\n",
        "\n",
        "    #save epoch samples\n",
        "    with open(savedir/f\"samples{epoch}.{config.source_lang}-{config.target_lang}.txt\", \"w\") as f:\n",
        "      for s, h in zip(stats[\"srcs\"], stats[\"hyps\"]):\n",
        "        f.write(f\"{s}\\t{h}\\n\")\n",
        "    score = bleu.score\n",
        "    # get best valid bleu    \n",
        "    if getattr(validate_and_save, \"best_bleu\", 0) < bleu.score:\n",
        "      validate_and_save.best_bleu = bleu.score\n",
        "      torch.save(check, savedir/f\"checkpoint_best.pt\")\n",
        "            \n",
        "    del_file = savedir / f\"checkpoint{epoch - config.keep_last_epochs}.pt\"\n",
        "    if del_file.exists():\n",
        "      del_file.unlink()\n",
        "    \n",
        "  return stats\n",
        "\n",
        "def try_load_checkpoint(model, optimizer=None, name=None):\n",
        "  name = name if name else \"checkpoint_last.pt\"\n",
        "  checkpath = Path(config.savedir)/name\n",
        "  if checkpath.exists():\n",
        "    check = torch.load(checkpath)\n",
        "    model.load_state_dict(check[\"model\"])\n",
        "    stats = check[\"stats\"]\n",
        "    step = \"unknown\"\n",
        "    if optimizer != None:\n",
        "      optimizer._step = step = check[\"optim\"][\"step\"]\n",
        "    logger.info(f\"loaded checkpoint {checkpath}: step={step} loss={stats['loss']} bleu={stats['bleu']}\")\n",
        "  else:\n",
        "    logger.info(f\"no checkpoints found at {checkpath}!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfeQTwGEMyIu"
      },
      "source": [
        "**train loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiUbilxtM6Fn",
        "outputId": "7a5b8ab3-5540-46ac-f37e-3df20271f9ca"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Aug  4 09:12:04 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    32W / 250W |   7935MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shRSnBDrM0Rw"
      },
      "source": [
        "model = model.to(device = device)\n",
        "criterion = criterion.to(device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeCOJEGNM_KO"
      },
      "source": [
        "**make prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KllXqrouNBDI"
      },
      "source": [
        "def generate_prediction(model, task, split = \"test\", outfile = \"./prediction.txt\"):\n",
        "  task.load_dataset(split=split, epoch=1)\n",
        "  itr = load_data_iterator(task, split, 1, config.max_tokens, config.num_workers).next_epoch_itr(shuffle=False)\n",
        "    \n",
        "  idxs = []\n",
        "  hyps = []\n",
        "\n",
        "  model.eval()\n",
        "  progress = tqdm.tqdm(itr, desc=f\"prediction\")\n",
        "  with torch.no_grad():\n",
        "    for i, sample in enumerate(progress):\n",
        "      # validation loss\n",
        "      sample = utils.move_to_cuda(sample, device=device)\n",
        "\n",
        "      # 進行推論\n",
        "      s, h, r = inference_step(sample, model)\n",
        "      # print(sample)\n",
        "      # print(h)\n",
        "      hyps.extend(h)\n",
        "      idxs.extend(list(sample['id']))\n",
        "            \n",
        "  # 根據 preprocess 時的順序排列\n",
        "  hyps = [x for _,x in sorted(zip(idxs,hyps))]\n",
        "    \n",
        "  with open(outfile, \"w\") as f:\n",
        "    for h in hyps:\n",
        "      f.write(h+\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ9tqIQiNXVy"
      },
      "source": [
        "**Back-translation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfc8qK6ZNUED"
      },
      "source": [
        "config = Namespace(\n",
        "  datadir = \"./DATA/data-bin/ted2020\",\n",
        "  savedir = \"./checkpoints/rnn_back\",\n",
        "  source_lang = \"zh\",\n",
        "  target_lang = \"en\",\n",
        "    \n",
        "  # cpu threads when fetching & processing data.\n",
        "  num_workers=2,  \n",
        "  # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
        "  max_tokens=8192,\n",
        "  accum_steps=2,\n",
        "    \n",
        "  # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
        "  lr_factor=2.,\n",
        "  lr_warmup=4000,\n",
        "    \n",
        "  # clipping gradient norm helps alleviate gradient exploding\n",
        "  clip_norm=1.0,\n",
        "    \n",
        "  # maximum epochs for training\n",
        "  max_epoch=40,\n",
        "  start_epoch=1,\n",
        "    \n",
        "  # beam size for beam search\n",
        "  beam=5, \n",
        "  # generate sequences of maximum length ax + b, where x is the source length\n",
        "  max_len_a=1.2, \n",
        "  max_len_b=10,\n",
        "  # when decoding, post process sentence by removing sentencepiece symbols.\n",
        "  post_process = \"sentencepiece\",\n",
        "    \n",
        "  # checkpoints\n",
        "  keep_last_epochs=5,\n",
        "  resume=None, # if resume from checkpoint name (under config.savedir)\n",
        "    \n",
        "  # logging\n",
        "  use_wandb=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ_w5mbPOhov",
        "outputId": "810fb6a3-a212-4976-899b-f43940574d95"
      },
      "source": [
        "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
        "\n",
        "## setup task\n",
        "task_cfg = TranslationConfig(\n",
        "  data=config.datadir,\n",
        "  source_lang=config.source_lang,\n",
        "  target_lang=config.target_lang,\n",
        "  train_subset=\"train\",\n",
        "  required_seq_len_multiple=8,\n",
        "  dataset_impl=\"mmap\",\n",
        "  upsample_primary=1,\n",
        ")\n",
        "task = TranslationTask.setup_task(task_cfg)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-04 09:12:10 | INFO | fairseq.tasks.translation | [zh] dictionary: 8000 types\n",
            "2021-08-04 09:12:10 | INFO | fairseq.tasks.translation | [en] dictionary: 8000 types\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUPQHGV-Ow5x",
        "outputId": "41658538-69ef-4132-db7f-ef1a39fe95f6"
      },
      "source": [
        "logger.info(\"loading data for epoch 1\")\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "task.load_dataset(split=\"valid\", epoch=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-04 09:12:11 | INFO | hw5.seq2seq | loading data for epoch 1\n",
            "2021-08-04 09:12:11 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020/train.en-zh.zh\n",
            "2021-08-04 09:12:11 | INFO | fairseq.data.data_utils | loaded 390,041 examples from: ./DATA/data-bin/ted2020/train.en-zh.en\n",
            "2021-08-04 09:12:11 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 train zh-en 390041 examples\n",
            "2021-08-04 09:12:11 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020/valid.en-zh.zh\n",
            "2021-08-04 09:12:11 | INFO | fairseq.data.data_utils | loaded 3,939 examples from: ./DATA/data-bin/ted2020/valid.en-zh.en\n",
            "2021-08-04 09:12:11 | INFO | fairseq.tasks.translation | ./DATA/data-bin/ted2020 valid zh-en 3939 examples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okDSYB4dO6At",
        "outputId": "28d9462f-28a7-40c8-813b-47276b5ff796"
      },
      "source": [
        "sample = task.dataset(\"valid\")[1]\n",
        "pprint.pprint(sample)\n",
        "pprint.pprint(\n",
        "  \"Source: \" + \\\n",
        "  task.source_dictionary.string(\n",
        "    sample['source'],\n",
        "    config.post_process,\n",
        "  )\n",
        ")\n",
        "pprint.pprint(\n",
        "  \"Target: \" + \\\n",
        "  task.target_dictionary.string(\n",
        "    sample['target'],\n",
        "    config.post_process,\n",
        "  )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'id': 1,\n",
            " 'source': tensor([ 140,  690,   28,  270,   45,  151, 1142,  660,  606,  369, 3114, 2434,\n",
            "        1434,  192,    2]),\n",
            " 'target': tensor([  18,   14,    6, 2234,   60,   19,   80,    5,  256,   16,  405, 1407,\n",
            "        1706,    7,    2])}\n",
            "'Source: 這實在就是我所做的--光學操控思想'\n",
            "\"Target: that's exactly what i do optical mind control .\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "porg1slfPLzS"
      },
      "source": [
        "logging.basicConfig(\n",
        "  format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "  datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "  level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
        "  stream=sys.stdout,\n",
        ")\n",
        "proj = \"hw5.seq2seq\"\n",
        "logger = logging.getLogger(proj)\n",
        "if config.use_wandb:\n",
        "  import wandb\n",
        "  wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn7UvrD5PNfp",
        "outputId": "0ea64b53-e470-4f1e-d9db-5d52b5ccc127"
      },
      "source": [
        "model = build_model(arch_args, task)\n",
        "logger.info(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-04 09:12:19 | INFO | hw5.seq2seq | Seq2Seq(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(8000, 1024, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(8000, 1024, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
            "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
            "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
            "    (output_projection): Linear(in_features=1024, out_features=8000, bias=False)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVRhtpwPw4h",
        "outputId": "8b52ae14-61b9-4359-ce11-483a50923534"
      },
      "source": [
        "sequence_generator = task.build_generator([model], config)\n",
        "model = model.to(device = device)\n",
        "criterion = criterion.to(device = device)\n",
        "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch = 1, max_tokens = 20, num_workers = 1, cached = False)\n",
        "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle = True)\n",
        "sample = next(demo_iter)\n",
        "sample"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-04 09:12:19 | WARNING | fairseq.tasks.fairseq_task | 2,532 samples have invalid sizes and will be skipped, max_positions=(20, 20), first few sample ids=[3525, 527, 1633, 76, 2861, 2415, 2890, 210, 880, 636]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': tensor([963]),\n",
              " 'net_input': {'prev_output_tokens': tensor([[   2,  554,   36,   38,    7,   55,   24,  155,    4,  278,  407, 1362,\n",
              "             26, 1011,   25,  153, 2055,    7,    1,    1,    1,    1,    1,    1]]),\n",
              "  'src_lengths': tensor([17]),\n",
              "  'src_tokens': tensor([[   1,    1,    1,    1,    1,    1,    1,    5,  971, 1132,  373,  160,\n",
              "            516,  315,  433,   33,    5, 3673, 2044,  339,  230,  102,  976,    2]])},\n",
              " 'nsentences': 1,\n",
              " 'ntokens': 18,\n",
              " 'target': tensor([[ 554,   36,   38,    7,   55,   24,  155,    4,  278,  407, 1362,   26,\n",
              "          1011,   25,  153, 2055,    7,    2,    1,    1,    1,    1,    1,    1]])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFvXhBLhQW4t",
        "outputId": "7627a81a-2585-453e-fbc8-65e55b4d6958"
      },
      "source": [
        "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
        "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
        "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
        "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
        "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
        "logger.info(\n",
        "  \"num. model params: {:,} (num. trained: {:,})\".format(\n",
        "    sum(p.numel() for p in model.parameters()),\n",
        "    sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "  )\n",
        ")\n",
        "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-04 09:12:19 | INFO | hw5.seq2seq | task: TranslationTask\n",
            "2021-08-04 09:12:19 | INFO | hw5.seq2seq | encoder: TransformerEncoder\n",
            "2021-08-04 09:12:19 | INFO | hw5.seq2seq | decoder: TransformerDecoder\n",
            "2021-08-04 09:12:19 | INFO | hw5.seq2seq | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2021-08-04 09:12:19 | INFO | hw5.seq2seq | optimizer: NoamOpt\n",
            "2021-08-04 09:12:19 | INFO | hw5.seq2seq | num. model params: 192,745,472 (num. trained: 192,745,472)\n",
            "2021-08-04 09:12:19 | INFO | hw5.seq2seq | max tokens per batch = 8192, accumulate steps = 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "vOTv3nmmQecE",
        "outputId": "e6cf096a-bfd4-4307-a83e-06be6db7ea1b"
      },
      "source": [
        "optimizer = NoamOpt(\n",
        "  model_size=arch_args.encoder_embed_dim, \n",
        "  factor=config.lr_factor, \n",
        "  warmup=config.lr_warmup, \n",
        "  optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
        "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
        "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4c1a34f750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542,
          "referenced_widgets": [
            "7a78679489e3429daf6037df265ee216",
            "bf54b330a55e4bf5b36a8a1e41363329",
            "c02c45839ec8418bab1f1a335d0cf2a1",
            "49cdba9b508b458384a66e81abade621",
            "86f73e0d31f848e3b9036f8ed6d4b415",
            "091c1376c46f42a5844efc8278500b78",
            "f0781d26a60541d882abc8b9692a9406",
            "07cbd524bdf24d1a98db7ebdb00bb760"
          ]
        },
        "id": "U0uBWUkTQuzA",
        "outputId": "a535c98e-7ebe-4cec-d496-2e51f89c99c7"
      },
      "source": [
        "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
        "try_load_checkpoint(model, optimizer, name=config.resume)\n",
        "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
        "  # train for one epoch\n",
        "  train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
        "  stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
        "  logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
        "  epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-08-04 09:13:28 | INFO | hw5.seq2seq | no checkpoints found at checkpoints/rnn_back/checkpoint_last.pt!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a78679489e3429daf6037df265ee216",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='train epoch 1', max=800.0, style=ProgressStyle(descriptio…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-491d32f92d7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mepoch_itr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_epoch_idx\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# train for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_itr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_itr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"end of epoch {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_itr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-66-9a897517f5e3>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_itr, model, task, criterion, optimizer, accum_steps)\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;31m#混和精度訓練\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mnet_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"net_input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mlprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-38-997b4aa88010>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_tokens, src_lengths, prev_output_tokens, return_all_hiddens)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m#Run the forward pass for an encoder-decoder model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     encoder_out = self.encoder(\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0msrc_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_all_hiddens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     )\n\u001b[1;32m     10\u001b[0m     logits, extra = self.decoder(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fairseq/models/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src_tokens, src_lengths, return_all_hiddens, token_embeddings)\u001b[0m\n\u001b[1;32m    430\u001b[0m                   \u001b[0mOnly\u001b[0m \u001b[0mpopulated\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mreturn_all_hiddens\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \"\"\"\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;31m# B x T x C -> T x B x C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fairseq/models/transformer.py\u001b[0m in \u001b[0;36mforward_embedding\u001b[0;34m(self, src_tokens, token_embedding)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtoken_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm_embedding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fairseq/modules/sinusoidal_positional_embedding.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, incremental_state, timestep, positions)\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         return (\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 15.90 GiB total capacity; 14.60 GiB already allocated; 37.75 MiB free; 14.96 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRGSr6XFTUbv"
      },
      "source": [
        "mono_dataset_name = 'mono'\n",
        "mono_prefix = Path(data_dir).absolute() / mono_dataset_name\n",
        "mono_prefix.mkdir(parents =True, exist_ok = True)\n",
        "\n",
        "urls = (\n",
        "    '\"https://onedrive.live.com/download?cid=3E549F3B24B238B4&resid=3E549F3B24B238B4%214986&authkey=AANUKbGfZx0kM80\"',\n",
        "# # If the above links die, use the following instead. \n",
        "#     \"https://www.csie.ntu.edu.tw/~r09922057/ML2021-hw5/ted_zh_corpus.deduped.gz\",\n",
        "# # If the above links die, use the following instead. \n",
        "#     \"https://mega.nz/#!vMNnDShR!4eHDxzlpzIpdpeQTD-htatU_C7QwcBTwGDaSeBqH534\",\n",
        ")\n",
        "file_names = (\n",
        "  'ted_zh_corpus.deduped.gz',\n",
        ")\n",
        "for u, f in zip(urls, file_names):\n",
        "  path = mono_prefix/f\n",
        "  if not path.exists():\n",
        "    if 'mega' in u:\n",
        "      !megadl{u} --path {path}\n",
        "    else:\n",
        "      !wget {u} -O {path}\n",
        "  else:\n",
        "    print(f'{f} is exist, skip downloading')\n",
        "  if path.suffix == \".tgz\":\n",
        "    !tar -xvf {path} -C {prefix}\n",
        "  elif path.suffix == \".zip\":\n",
        "    !unzip -o {path} -d {prefix}\n",
        "  elif path.suffix == \".gz\":\n",
        "    !gzip -fkd {path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmncTEg1VbyT"
      },
      "source": [
        "!pip install jieba\n",
        "import jieba\n",
        "import random\n",
        "random.seed(901027)\n",
        "\n",
        "def rand_delete(line, p=0.1):\n",
        "  remove = []\n",
        "  for i in range(len(line) - 1):\n",
        "    if random.randint(0, 100) < prob*100:\n",
        "      remove.append(line[i])\n",
        "\n",
        "  for x in remove:\n",
        "    line.remove(x)\n",
        "    \n",
        "  return ''.join(line)\n",
        "\n",
        "def rand_switch(line, n_times):\n",
        "  idx = range(len(line) - 1)\n",
        "  for i in range(n_times):\n",
        "    i1, i2 = random.sample(idx, 2)\n",
        "    line[i1], line[i2] = line[i2], line[i1]\n",
        "    \n",
        "  return ''.join(line)\n",
        "\n",
        "with open(f'{mono_prefix}/ted_zh_corpus.deduped', 'r') as f_read:\n",
        "  with open(f'{mono_prefix}/ted_zh_corpus.deduped1', 'w') as f_write:\n",
        "    line = f_read.readline()\n",
        "    line = jieba.cut(line, cut_all = False)\n",
        "    line = \" \".join(line)\n",
        "    line = line.split(\" \")\n",
        "    num = random.randint(0,1)\n",
        "    if num == 0:\n",
        "      line = rand_delete(line, p=0.1)\n",
        "    else:\n",
        "      times = (len(line) // 10) + 1\n",
        "      line = rand_switch(line, times)\n",
        "    f_write.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js0H71z3Vs_X"
      },
      "source": [
        "**Clean Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu39f-cxVvJg"
      },
      "source": [
        "def mono_clean_corpus(prefix, max_len=1000, min_len=1):\n",
        "  if Path(f'{prefix}.clean').exists():\n",
        "    print(f'{prefix}.clean exists. skipping clean.')\n",
        "    return\n",
        "  with open(f'{prefix}', 'r') as l1_in_f:\n",
        "    with open(f'{prefix}.clean', 'w') as l1_out_f:\n",
        "      for s1 in l1_in_f:\n",
        "        s1 = s1.strip()\n",
        "        s1 = clean_s(s1, \"zh\")\n",
        "        s1_len = len_s(s1, \"zh\")\n",
        "        if min_len > 0: # remove short sentence\n",
        "          if s1_len < min_len:\n",
        "            continue\n",
        "        if max_len > 0: # remove long sentence\n",
        "          if s1_len > max_len:\n",
        "            continue\n",
        "        print(s1, file=l1_out_f)\n",
        "mono_clean_corpus(f'{mono_prefix}/ted_zh_corpus.deduped')\n",
        "mono_clean_corpus(f'{mono_prefix}/ted_zh_corpus.deduped1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPoMJTbbV7Ua"
      },
      "source": [
        "import sentencepiece as spm\n",
        "vocab_size = 8000\n",
        "if (mono_prefix/f'spm{vocab_size}.model').exists():\n",
        "  print(f'{mono_prefix}/spm{vocab_size}.model exists. skipping spm_train.')\n",
        "else:\n",
        "  spm.SentencePieceTrainer.train(\n",
        "    input=f'{mono_prefix}/ted_zh_corpus.deduped.clean',\n",
        "    model_prefix=mono_prefix/f'spm{vocab_size}',\n",
        "    vocab_size=vocab_size,\n",
        "    character_coverage=1,\n",
        "    model_type='unigram', # 'bpe' works as well\n",
        "    input_sentence_size=1e6,\n",
        "    shuffle_input_sentence=True,\n",
        "    normalization_rule_name='nmt_nfkc_cf',\n",
        "  )\n",
        "\n",
        "spm_model = spm.SentencePieceProcessor(model_file=str(mono_prefix/f'spm{vocab_size}.model'))\n",
        "\n",
        "out_path = mono_prefix/f'mono.tok.zh'\n",
        "if out_path.exists():\n",
        "  print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "else:\n",
        "  with open(mono_prefix/f'mono.tok.zh', 'w') as out_f:\n",
        "    with open(mono_prefix/f'ted_zh_corpus.deduped.clean', 'r') as in_f:\n",
        "      for line in in_f:\n",
        "        line = line.strip()\n",
        "        tok = spm_model.encode(line, out_type=str)\n",
        "        print(' '.join(tok), file=out_f)\n",
        "\n",
        "out_path = mono_prefix/f'mono.tok1.zh'\n",
        "if out_path.exists():\n",
        "  print(f\"{out_path} exists. skipping spm_encode.\")\n",
        "else:\n",
        "  with open(mono_prefix/f'mono.tok.zh', 'w') as out_f:\n",
        "    with open(mono_prefix/f'ted_zh_corpus.deduped.clean', 'r') as in_f:\n",
        "      for line in in_f:\n",
        "        line = line.strip()\n",
        "        tok = spm_model.encode(line, out_type=str)\n",
        "        print(' '.join(tok), file=out_f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFEmfDy3WLKO"
      },
      "source": [
        "!head {mono_prefix}/ted_zh_corpus.deduped.clean -n 5\n",
        "!head {mono_prefix}/mono.tok.zh -n 5\n",
        "\n",
        "!head {mono_prefix}/ted_zh_corpus.deduped1.clean -n 5\n",
        "!head {mono_prefix}/mono.tok1.zh -n 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGGVSlbJWWla"
      },
      "source": [
        "**Binarize**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfa2SdV5WNvP"
      },
      "source": [
        "en_path = './DATA/rawdata/mono/mono.tok.en'\n",
        "zh_path = './DATA/rawdata/mono/mono.tok.zh'\n",
        "en_path1 = './DATA/rawdata/mono/mono.tok1.en'\n",
        "zh_path1 = './DATA/rawdata/mono/mono.tok1.zh'\n",
        "def file_len(fname):\n",
        "  with open(fname) as f:\n",
        "    for i, l in enumerate(f):\n",
        "      pass\n",
        "  return i + 1\n",
        "length = file_len(zh_path)\n",
        "with open(en_path, \"w\") as f:\n",
        "  for i in tqdm.tqdm(range(length)):\n",
        "    f.write(\"▁ 。\\n\")\n",
        "\n",
        "length = file_len(zh_path1)\n",
        "with open(en_path1, \"w\") as f:\n",
        "  for i in tqdm.tqdm(range(length)):\n",
        "    f.write(\"▁ 。\\n\")\n",
        "# input(\"QQQQQ\")\n",
        "\n",
        "binpath = Path('./DATA/data-bin', mono_dataset_name)\n",
        "src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
        "tgt_dict_file = src_dict_file\n",
        "monopref = str(mono_prefix/\"mono.tok\") # whatever filepath you get after applying subword tokenization\n",
        "if binpath.exists():\n",
        "  print(binpath, \"exists, will not overwrite!\")\n",
        "else:\n",
        "  !python -m fairseq_cli.preprocess\\\n",
        "    --source-lang 'zh'\\\n",
        "    --target-lang 'en'\\\n",
        "    --trainpref {monopref}\\\n",
        "    --destdir {binpath}\\\n",
        "    --srcdict {src_dict_file}\\\n",
        "    --tgtdict {tgt_dict_file}\\\n",
        "    --workers 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_4mpDnCWZkV"
      },
      "source": [
        "**Generate Back-translation data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-frE7TkWZBo"
      },
      "source": [
        "# 將 binarized data 加入原本的資料夾中並用一個 split_name 取名\n",
        "# ex. ./DATA/data-bin/ted2020/\\[split_name\\].zh-en.\\[\"en\", \"zh\"\\].\\[\"bin\", \"idx\"\\]\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.zh.bin ./DATA/data-bin/ted2020/mono.zh-en.zh.bin\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.zh.idx ./DATA/data-bin/ted2020/mono.zh-en.zh.idx\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.en.bin ./DATA/data-bin/ted2020/mono.zh-en.en.bin\n",
        "!cp ./DATA/data-bin/mono/train.zh-en.en.idx ./DATA/data-bin/ted2020/mono.zh-en.en.idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzPUhd43Wgn0"
      },
      "source": [
        "# hint: 用反向模型在 split='mono' 上進行預測，生成 prediction_file\n",
        "# generate_prediction( ... ,split=... ,outfile=... )\n",
        "# averaging a few checkpoints can have a similar effect to ensemble\n",
        "try_load_checkpoint(model, name=\"checkpoint_best.pt\")\n",
        "\n",
        "generate_prediction(model, task, split=\"mono\", outfile=\"./back_pred.txt\" )\n",
        "print(\"Finished back predicting!!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98n3KfrTWjmm"
      },
      "source": [
        "**Generate new dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9OCR3PNWlqg"
      },
      "source": [
        "# 合併剛剛生成的 prediction_file (.en) 以及中文 mono.zh (.zh)\n",
        "# \n",
        "# hint: 在此用剛剛的 spm model 對 prediction_file 進行切斷詞\n",
        "# spm_model.encode(line, out_type=str)\n",
        "# output: ./DATA/rawdata/mono/mono.tok.en & mono.tok.zh\n",
        "spm_model = spm.SentencePieceProcessor(model_file=str(prefix/f'spm{vocab_size}.model'))\n",
        "\n",
        "out_path = mono_prefix/f'mono.tok.en'\n",
        "with open(mono_prefix/f'mono.tok.en', 'w') as out_f:\n",
        "  with open('./back_pred.txt', 'r') as in_f:\n",
        "    for line in in_f:\n",
        "      line = line.strip()\n",
        "      tok = spm_model.encode(line, out_type=str)\n",
        "      print(' '.join(tok), file=out_f)\n",
        "# input()\n",
        "\n",
        "# hint: 在此用 fairseq 把這些檔案再 binarize\n",
        "binpath = Path('./DATA/data-bin/synthetic')\n",
        "src_dict_file = './DATA/data-bin/ted2020/dict.en.txt'\n",
        "tgt_dict_file = src_dict_file\n",
        "monopref = \"./DATA/rawdata/mono/mono.tok\" # or whatever path after applying subword tokenization, w/o the suffix (.zh/.en)\n",
        "!python -m fairseq_cli.preprocess\\\n",
        "  --source-lang 'zh'\\\n",
        "  --target-lang 'en'\\\n",
        "  --trainpref {monopref}\\\n",
        "  --destdir {binpath}\\\n",
        "  --srcdict {src_dict_file}\\\n",
        "    --tgtdict {tgt_dict_file}\\\n",
        "  --workers 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDQH4OSoWsO0"
      },
      "source": [
        "# 這裡用剛剛準備的檔案合併原先 ted2020 來生成最終 back-translation 的資料\n",
        "!cp -r ./DATA/data-bin/ted2020/ ./DATA/data-bin/ted2020_with_mono/\n",
        "\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.zh.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.bin\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.zh.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.zh.idx\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.en.bin ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.bin\n",
        "!cp ./DATA/data-bin/synthetic/train.zh-en.en.idx ./DATA/data-bin/ted2020_with_mono/train1.en-zh.en.idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JwRfitlWuE4"
      },
      "source": [
        "**re-train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NxLoVLxWvl1"
      },
      "source": [
        "config = Namespace(\n",
        "  datadir = \"./DATA/data-bin/ted2020_with_mono\",\n",
        "  savedir = \"./checkpoints/rnn_bt\",\n",
        "  source_lang = \"en\",\n",
        "  target_lang = \"zh\",\n",
        "    \n",
        "  # cpu threads when fetching & processing data.\n",
        "  num_workers=2,  \n",
        "  # batch size in terms of tokens. gradient accumulation increases the effective batchsize.\n",
        "  max_tokens=8192,\n",
        "  accum_steps=2,\n",
        "    \n",
        "  # the lr s calculated from Noam lr scheduler. you can tune the maximum lr by this factor.\n",
        "  lr_factor=2.,\n",
        "  lr_warmup=4000,\n",
        "    \n",
        "  # clipping gradient norm helps alleviate gradient exploding\n",
        "  clip_norm=1.0,\n",
        "    \n",
        "  # maximum epochs for training\n",
        "  max_epoch=150,\n",
        "  start_epoch=1,\n",
        "  early_stop=5,\n",
        "    \n",
        "  # beam size for beam search\n",
        "  beam=5, \n",
        "  # generate sequences of maximum length ax + b, where x is the source length\n",
        "  max_len_a=1.2, \n",
        "  max_len_b=10,\n",
        "  # when decoding, post process sentence by removing sentencepiece symbols.\n",
        "  post_process = \"sentencepiece\",\n",
        "    \n",
        "  # checkpoints\n",
        "  keep_last_epochs=5,\n",
        "  resume=None, # if resume from checkpoint name (under config.savedir)\n",
        "    \n",
        "  # logging\n",
        "  use_wandb=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wOCGGIjXL1a"
      },
      "source": [
        "from fairseq.tasks.translation import TranslationConfig, TranslationTask\n",
        "\n",
        "## setup task\n",
        "task_cfg = TranslationConfig(\n",
        "  data=config.datadir,\n",
        "  source_lang=config.source_lang,\n",
        "  target_lang=config.target_lang,\n",
        "  train_subset=\"train\",\n",
        "  required_seq_len_multiple=8,\n",
        "  dataset_impl=\"mmap\",\n",
        "  upsample_primary=1,\n",
        ")\n",
        "task = TranslationTask.setup_task(task_cfg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw3jCfhSXZ07"
      },
      "source": [
        "logger.info(\"loading data for epoch 1\")\n",
        "task.load_dataset(split=\"train\", epoch=1, combine=True) # combine if you have back-translation data.\n",
        "task.load_dataset(split=\"valid\", epoch=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-FcA4O9XgJe"
      },
      "source": [
        "sample = task.dataset(\"valid\")[1]\n",
        "pprint.pprint(sample)\n",
        "pprint.pprint(\n",
        "  \"Source: \" + \\\n",
        "  task.source_dictionary.string(\n",
        "    sample['source'],\n",
        "    config.post_process,\n",
        "  )\n",
        ")\n",
        "pprint.pprint(\n",
        "  \"Target: \" + \\\n",
        "  task.target_dictionary.string(\n",
        "    sample['target'],\n",
        "    config.post_process,\n",
        "  )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LL3bBHw6Xncx"
      },
      "source": [
        "logging.basicConfig(\n",
        "  format=\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\",\n",
        "  datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        "  level=\"INFO\", # \"DEBUG\" \"WARNING\" \"ERROR\"\n",
        "  stream=sys.stdout,\n",
        ")\n",
        "proj = \"hw5.seq2seq\"\n",
        "logger = logging.getLogger(proj)\n",
        "if config.use_wandb:\n",
        "  import wandb\n",
        "  wandb.init(project=proj, name=Path(config.savedir).stem, config=config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrdGsGE7Xtrf"
      },
      "source": [
        "sequence_generator = task.build_generator([model], config)\n",
        "model = model.to(device = device)\n",
        "criterion = criterion.to(device = device)\n",
        "demo_epoch_obj = load_data_iterator(task, \"valid\", epoch = 1, max_tokens = 20, num_workers = 1, cached = False)\n",
        "demo_iter = demo_epoch_obj.next_epoch_itr(shuffle = True)\n",
        "sample = next(demo_iter)\n",
        "sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aX5BB3JXxhY"
      },
      "source": [
        "logger.info(\"task: {}\".format(task.__class__.__name__))\n",
        "logger.info(\"encoder: {}\".format(model.encoder.__class__.__name__))\n",
        "logger.info(\"decoder: {}\".format(model.decoder.__class__.__name__))\n",
        "logger.info(\"criterion: {}\".format(criterion.__class__.__name__))\n",
        "logger.info(\"optimizer: {}\".format(optimizer.__class__.__name__))\n",
        "logger.info(\n",
        "  \"num. model params: {:,} (num. trained: {:,})\".format(\n",
        "    sum(p.numel() for p in model.parameters()),\n",
        "    sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "  )\n",
        ")\n",
        "logger.info(f\"max tokens per batch = {config.max_tokens}, accumulate steps = {config.accum_steps}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_pExjUmX4jK"
      },
      "source": [
        "optimizer = NoamOpt(\n",
        "  model_size=arch_args.encoder_embed_dim, \n",
        "  factor=config.lr_factor, \n",
        "  warmup=config.lr_warmup, \n",
        "  optimizer=torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9, weight_decay=0.0001))\n",
        "plt.plot(np.arange(1, 100000), [optimizer.rate(i) for i in range(1, 100000)])\n",
        "plt.legend([f\"{optimizer.model_size}:{optimizer.warmup}\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PupiDF-KX7bb"
      },
      "source": [
        "epoch_itr = load_data_iterator(task, \"train\", config.start_epoch, config.max_tokens, config.num_workers)\n",
        "try_load_checkpoint(model, optimizer, name=config.resume)\n",
        "score = 0\n",
        "best_score = 0\n",
        "cur = 0\n",
        "while epoch_itr.next_epoch_idx <= config.max_epoch:\n",
        "  # train for one epoch\n",
        "  if score > best_score:\n",
        "    cur += 1\n",
        "  if cur > config.early_stop:\n",
        "    break\n",
        "  train_one_epoch(epoch_itr, model, task, criterion, optimizer, config.accum_steps)\n",
        "  stats = validate_and_save(model, task, criterion, optimizer, epoch=epoch_itr.epoch)\n",
        "  logger.info(\"end of epoch {}\".format(epoch_itr.epoch))    \n",
        "  epoch_itr = load_data_iterator(task, \"train\", epoch_itr.next_epoch_idx, config.max_tokens, config.num_workers)\n",
        "  if score > best_score:\n",
        "    best_score = score\n",
        "    cur = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlOI4otyYXf5"
      },
      "source": [
        "# hint: 用反向模型在 split='mono' 上進行預測，生成 prediction_file\n",
        "# generate_prediction( ... ,split=... ,outfile=... )\n",
        "# averaging a few checkpoints can have a similar effect to ensemble\n",
        "checkdir=config.savedir\n",
        "!python ./fairseq/scripts/average_checkpoints.py \\\n",
        "--inputs {checkdir} \\\n",
        "--num-epoch-checkpoints 5 \\\n",
        "--output {checkdir}/avg_last_5_checkpoint.pt\n",
        "try_load_checkpoint(model, name=\"checkpoint_best.pt\")\n",
        "\n",
        "generate_prediction(model, task)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}